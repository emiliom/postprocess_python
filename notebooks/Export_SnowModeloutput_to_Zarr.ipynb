{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract SWE data from SnowModel output and export to zarr in time increments\n",
    "3/31/2020. https://github.com/emiliom\n",
    "\n",
    "This notebook implements a single scheme to export to zarr on either the local file system, MinIO (local S3) or AWS S3, based on user choice (`FS_type` variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes, background, TO-DOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies, TO-DOs\n",
    "- *Ongoing.* Add more attributes, to enrich the metadata\n",
    "    - Global attributes, including ones matching with CF conventions.\n",
    "    - Variable attributes, including CF standard name\n",
    "- DONE. Read a portion of the file at a time. I can't read it all at once b/c it's larger than my available laptop memory (23 GB vs 15.4 GB). But instead of first writing to incremental netcdf files, it's more practical to read the binary data in the same time segments, and write to zarr instead, concatenating along the time dimension\n",
    "- DONE. Keeping default `NaN` for `_FillValue`. xrray auto added `'_FillValue': nan` to the `encoding` of the `swe` array. See [xarray #1598](https://github.com/pydata/xarray/issues/1598) and [this](https://cf-trac.llnl.gov/trac/ticket/52) for background discussions. CF/netCDF don't forbid NaN as `_FillValue`. [NCEI NetCDF Template discourages it](https://www.nodc.noaa.gov/data/formats/netcdf/v2.0/faq.html).\n",
    "- DONE. Add dimension variables (time, x, y), with corresponding attributes.\n",
    "- DONE. Create scheme that accepts a specified/desired number of time steps in each time increment (tsubset), and from it generates a new sequence specifying the number of time steps in each increment. The last increment (last sequence element) may have a smaller number of time steps\n",
    "\n",
    "### References for NetCDF CF/ACDD conventions\n",
    "- https://www.nodc.noaa.gov/data/formats/netcdf/v2.0/\n",
    "- https://www.nodc.noaa.gov/data/formats/netcdf/v2.0/grid.cdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import math\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import dask\n",
    "import s3fs\n",
    "import boto3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.15.0', '2.4.0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.__version__, zarr.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dpth = Path(\"/usr/mayorgadat/workmain/aarendt/CSO/projectwork/snow_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set XYT dimensions and binary blob file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##USER INPUTS## - most of this can be read directly from the .ctl file or the .par file \n",
    "#-> need to read in text file\n",
    "\n",
    "# model filename\n",
    "inFile = 'swed.gdat'\n",
    "\n",
    "# start date\n",
    "st = \"2014-10-01\"\n",
    "# end date\n",
    "ed = \"2019-09-29\"\n",
    "# number of timesteps in model run \n",
    "timesteps = 365 * 5\n",
    "\n",
    "# from .ctl file\n",
    "nx = 1382 # number of cells in the x dimension\n",
    "ny = 2476 # number of cells in the y dimension\n",
    "xll = 487200\n",
    "yll = 4690100\n",
    "clsz = 100 # cellsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare derived coordinate (XYT) arrays and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_increments(timesteps, target_increments):\n",
    "    nts_increments = [target_increments] * int(timesteps / target_increments)\n",
    "    nts_increments.append(timesteps % target_increments)\n",
    "    nts_increments = np.array(nts_increments)\n",
    "    nts_increment_offsets = nts_increments.cumsum() - nts_increments\n",
    "    \n",
    "    return nts_increments, nts_increment_offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geographical projection is UTM Zone 12N, `epsg:32612`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easting (X) and Northing (Y) arrays\n",
    "easting_x = np.arange(xll, xll+nx*clsz, clsz)\n",
    "northing_y = np.arange(yll, yll+ny*clsz, clsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_days = pd.date_range(st, periods=timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1382, 2476, 1825)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(easting_x), len(northing_y), timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2014-10-01 00:00:00', freq='D'),\n",
       " Timestamp('2019-09-29 00:00:00', freq='D'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_days[0], time_days[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workhorse functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_as_xrds_tsubset(fpath, time, y, x, nts, nts_offset=0, minval=0):\n",
    "    \"\"\"open grads model output file and output a \n",
    "    metadata enriched xarray dataset\"\"\"\n",
    "\n",
    "    nx, ny = len(x), len(y)\n",
    "    # count : int. Number of items to read. -1 means all items (i.e., the complete file).\n",
    "    numpy_data = np.fromfile(\n",
    "        fpath, \n",
    "        dtype=np.float32, \n",
    "        count=nts*ny*nx,\n",
    "        offset=4*nts_offset*ny*nx  # offset is in bytes\n",
    "    ).reshape((nts, ny, nx))\n",
    "    # Switching to using valid_min attribute instead\n",
    "    # numpy_data = numpy_data.clip(min=minval)\n",
    "\n",
    "    time_subset = time[nts_offset:nts_offset+nts]\n",
    "\n",
    "    # TODO: Generalize attribute assignements to use a dictionary (or OrderedDict)\n",
    "    #  to be passed to this function\n",
    "\n",
    "    # Convert data to xarray DataArray, then add variable attributes\n",
    "    datavar = xr.DataArray(\n",
    "        numpy_data,\n",
    "        dims=('time', 'y', 'x'), \n",
    "        coords={'time': time_subset, 'y': y, 'x': x}\n",
    "    )\n",
    "    datavar.attrs['long_name'] = 'Snow Water Equivalent'\n",
    "    datavar.attrs['standard_name'] = 'lwe_thickness_of_surface_snow_amount'\n",
    "    datavar.attrs['units'] = 'meters'\n",
    "    datavar.attrs['valid_min'] = float(minval)\n",
    "\n",
    "    ## Create xarray Dataset, including dimension variables\n",
    "    d = OrderedDict()\n",
    "    d['time'] = ('time', time_subset)\n",
    "    d['x'] = ('x', x)\n",
    "    d['y'] = ('y', y)\n",
    "    d['swe'] = datavar\n",
    "    ds = xr.Dataset(d)\n",
    "\n",
    "    # Add global attributes\n",
    "    ds.attrs['description'] = \"SnowModel model run, SWE variable only\"\n",
    "    ds.attrs['CRS'] = \"UTM Zone 12N, EPSG:32612\"\n",
    "\n",
    "    # Add dimension/coordinate variable attributes\n",
    "    ds.time.attrs['standard_name'] = \"time\"\n",
    "    ds.time.attrs['axis'] = \"T\"\n",
    "\n",
    "    ds.x.attrs['long_name'] = \"Easting\"\n",
    "    ds.x.attrs['units'] = \"meters\"\n",
    "    ds.x.attrs['axis'] = \"X\"\n",
    "\n",
    "    ds.y.attrs['long_name'] = \"Northing\"\n",
    "    ds.y.attrs['units'] = \"meters\"\n",
    "    ds.y.attrs['axis'] = \"Y\"\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_increment_to_zarr(fs_type, zarrstore, ds, chunk, is_first=False):\n",
    "    \"\"\"Chunk and export to Zarr\"\"\"\n",
    "    \n",
    "    chunked_ds = ds.chunk(chunk)\n",
    "    \n",
    "    compute = False if fs_type.endswith('s3') else True\n",
    "    if is_first:\n",
    "        delayed_store = chunked_ds.to_zarr(\n",
    "            store=zarrstore,\n",
    "            mode='w',\n",
    "            consolidated=True,\n",
    "            compute=compute\n",
    "        )\n",
    "    else:\n",
    "        delayed_store = chunked_ds.to_zarr(\n",
    "            store=zarrstore,\n",
    "            mode='a',\n",
    "            consolidated=True,\n",
    "            append_dim=\"time\",\n",
    "            compute=compute\n",
    "        )\n",
    "    \n",
    "    if fs_type.endswith('s3'):\n",
    "        delayed_store.persist(retries=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read binary blob data incrementally and export to Zarr incrementally\n",
    "Iterate over nts_increments and nts_increment_offsets, generate xarray Dataset, and write (write first one, then append) each increment to zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for export to Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options: localfs, localminio_s3, aws_s3\n",
    "FS_type, bucket = \"aws_s3\", \"snowmodel\"\n",
    "\n",
    "# Use -1 to process the entire dataset\n",
    "timesteps_to_process = -1\n",
    "\n",
    "# should nts_target_increment be the same as the chunk time size?\n",
    "\n",
    "# Optimizing for whole-spatial-domain queries\n",
    "# zarrds = \"swe_run_a-geo.zarr\"\n",
    "# nts_target_increment = 4 \n",
    "# chunk = {'time': 4}\n",
    "\n",
    "# Optimizing for cell time-series queries\n",
    "zarrds = \"swe_run_a-ts.zarr\"\n",
    "nts_target_increment = 460\n",
    "chunk = {'time': 460, 'x': 150, 'y': 150}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute incremental read and export to Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([460, 460, 445]), array([   0,  460,  920, 1380]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nts_increments, nts_increment_offsets = time_increments(timesteps, nts_target_increment)\n",
    "\n",
    "nts_increments[-3:], nts_increment_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1825, 4, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(time_days), len(nts_increments), len(nts_increment_offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic strategy to export to local zarr, MinIO zarr, or AWS S3 zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FS_type == 'localfs':\n",
    "    # Looks like `open_zarr` doesn't accept pathlib Paths, so convert to str\n",
    "    zarrstore = str(base_dpth / bucket / zarrds)\n",
    "elif FS_type == 'localminio_s3':\n",
    "    FS = s3fs.S3FileSystem(\n",
    "        key='minioadmin',\n",
    "        secret='minioadmin',\n",
    "        client_kwargs={\"endpoint_url\": \"http://172.17.0.2:9000\"}\n",
    "    )\n",
    "    zarrstore = s3fs.S3Map(f\"{bucket}/{zarrds}\", s3=FS)\n",
    "elif FS_type == 'aws_s3':\n",
    "    # Use stored credentials file, ~/.aws/credentials\n",
    "    # Can also pass the credentials here explicitly via parameters\n",
    "    # aws_access_key_id and aws_secret_access_key, but that's not safe\n",
    "    aws_session = boto3.Session(\n",
    "        profile_name='cso',\n",
    "        aws_session_token=None,\n",
    "        region_name='us-west-2'\n",
    "    )\n",
    "    FS = s3fs.S3FileSystem(session=aws_session)\n",
    "    zarrstore = s3fs.S3Map(f\"{bucket}/{zarrds}\", s3=FS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-31T20:26:20Z\n",
      "**** Data increment: index 0 - 459, 2014-10-01 00:00:00 - 2016-01-03 00:00:00\n",
      "CPU times: user 1min 43s, sys: 22.7 s, total: 2min 6s\n",
      "Wall time: 22min 36s\n",
      "**** Data increment: index 460 - 919, 2016-01-04 00:00:00 - 2017-04-07 00:00:00\n",
      "CPU times: user 1min 41s, sys: 22.4 s, total: 2min 3s\n",
      "Wall time: 25min 12s\n",
      "**** Data increment: index 920 - 1379, 2017-04-08 00:00:00 - 2018-07-11 00:00:00\n",
      "CPU times: user 1min 45s, sys: 24 s, total: 2min 9s\n",
      "Wall time: 25min 36s\n",
      "**** Data increment: index 1380 - 1824, 2018-07-12 00:00:00 - 2019-09-29 00:00:00\n",
      "CPU times: user 1min 43s, sys: 16.8 s, total: 2min\n",
      "Wall time: 19min 46s\n",
      "2020-03-31T22:00:35Z\n"
     ]
    }
   ],
   "source": [
    "print(dt.strftime(dt.utcnow(), '%Y-%m-%dT%H:%M:%SZ'))\n",
    "\n",
    "if timesteps_to_process < 0:\n",
    "    nts_increments_to_process = len(nts_increments)\n",
    "else:\n",
    "    # rounds up to the next complete increment, not just the number of timesteps\n",
    "    nts_increments_to_process = math.floor(timesteps_to_process / nts_target_increment + 1)\n",
    "\n",
    "for nts_i in range(nts_increments_to_process):\n",
    "    # Read increment from the binary blob\n",
    "    ds = read_as_xrds_tsubset(inFile, time_days, northing_y, easting_x,\n",
    "                              nts_increments[nts_i], \n",
    "                              nts_offset=nts_increment_offsets[nts_i], \n",
    "                              minval=0)\n",
    "    \n",
    "    print(\n",
    "        \"**** Data increment: index {idx_start} - {idx_end}, {date_start} - {date_end}\".format(\n",
    "            idx_start=nts_increment_offsets[nts_i],\n",
    "            idx_end=nts_increment_offsets[nts_i]+nts_increments[nts_i]-1,\n",
    "            date_start=time_days[nts_increment_offsets[nts_i]],\n",
    "            date_end=time_days[nts_increment_offsets[nts_i]+nts_increments[nts_i]-1]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Export increment to zarr\n",
    "    # print(dt.strftime(dt.utcnow(), '%Y-%m-%dT%H:%M:%SZ'))\n",
    "    is_first = True if nts_i == 0 else False        \n",
    "    %time ds_increment_to_zarr(FS_type, zarrstore, ds, chunk, is_first)\n",
    "    \n",
    "print(dt.strftime(dt.utcnow(), '%Y-%m-%dT%H:%M:%SZ'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing times for writing the whole 25 GB dataset to zarr\n",
    "- **To zarr on AWS**\n",
    "    - 3/31: 94 minutes, using `model_increment = 460` and `chunk= {'time': 460, 'x': 150, 'y': 150}`. Increments were 20-23 min.\n",
    "    - 3/31: 4 hr 32 min, using `model_increment = 4` and `chunk= {'time': 4}`. Increments were 5 - 57 secs\n",
    "- **To zarr on local file system**\n",
    "    - 3/30 7:30pm: 2 min, using `chunk = {'time': 400, 'x': 200, 'y': 200}`. Each increment took 7.7 - 15 seconds. The size of the chunk files is 5 - 45 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read zarr dataset back, for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zds = xr.open_zarr(\n",
    "    store=zarrstore, \n",
    "    consolidated=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:  (time: 1825, x: 1382, y: 2476)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 2014-10-01 2014-10-02 ... 2019-09-29\n",
       "  * x        (x) int64 487200 487300 487400 487500 ... 625100 625200 625300\n",
       "  * y        (y) int64 4690100 4690200 4690300 ... 4937400 4937500 4937600\n",
       "Data variables:\n",
       "    swe      (time, y, x) float32 dask.array&lt;chunksize=(460, 150, 150), meta=np.ndarray&gt;\n",
       "Attributes:\n",
       "    CRS:          UTM Zone 12N, EPSG:32612\n",
       "    description:  SnowModel model run, SWE variable only</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (time: 1825, x: 1382, y: 2476)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 2014-10-01 2014-10-02 ... 2019-09-29\n",
       "  * x        (x) int64 487200 487300 487400 487500 ... 625100 625200 625300\n",
       "  * y        (y) int64 4690100 4690200 4690300 ... 4937400 4937500 4937600\n",
       "Data variables:\n",
       "    swe      (time, y, x) float32 dask.array<chunksize=(460, 150, 150), meta=np.ndarray>\n",
       "Attributes:\n",
       "    CRS:          UTM Zone 12N, EPSG:32612\n",
       "    description:  SnowModel model run, SWE variable only"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunks': (460, 150, 150),\n",
       " 'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0),\n",
       " 'filters': None,\n",
       " '_FillValue': nan,\n",
       " 'dtype': dtype('float32')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zds.swe.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'long_name': 'Snow Water Equivalent',\n",
       " 'standard_name': 'lwe_thickness_of_surface_snow_amount',\n",
       " 'units': 'meters',\n",
       " 'valid_min': 0.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zds.swe.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the attributes using zarr directly, *after* creating the zarr dataset\n",
    "Use this code to modify global and variable attributes of existing zarr datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zarr.open_consolidated doesn't allow changing the metadata\n",
    "# zstore = zarr.open_consolidated(store=zarrstore, mode='r+')\n",
    "\n",
    "#zstore = zarr.open(store=zarrstore, mode='r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zstore.attrs['new_appended_attr'] = 'my appended attribute'\n",
    "# zstore.swe.attrs['swe_append_attr'] = 'my swe appended attribute'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must pass the zarr store path, not the opened zstore\n",
    "# zarr.consolidate_metadata(zarrstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snowmodelaws]",
   "language": "python",
   "name": "conda-env-snowmodelaws-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
