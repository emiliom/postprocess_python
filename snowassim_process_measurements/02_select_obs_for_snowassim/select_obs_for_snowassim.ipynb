{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules and packages successfully loaded.\n",
      "40 =high\n",
      "402 =low\n",
      "9 =band_0250\n",
      "4 =band_0500\n",
      "67 =band_0750\n",
      "314 =band_1000\n",
      "15 =band_1250\n",
      "25 =band_gt1250\n",
      "0 =early\n",
      "442 =late\n",
      "396 =winter\n",
      "232 =spring\n",
      "210 =peak\n",
      "206 =sp1000\n",
      "26 =spgt1000\n",
      "186 =apr\n",
      "46 =may\n"
     ]
    }
   ],
   "source": [
    "###############   Select Obs for SnowAssim   #########################\n",
    "\n",
    "# This script takes in the entire dataset of observations from the Thompson Pass \n",
    "# modeling domain, after some preprocessing steps. \n",
    "\n",
    "# Inputs: a modified CSV of all of the yearly observations, originally downloaded from CSO website.\n",
    "# Outputs: a series of .dat files that are formatted to be the input for SnowAssim \n",
    "\n",
    "# Import some things\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "from math import pi\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "%matplotlib inline\n",
    "import random\n",
    "from random import sample\n",
    "\n",
    "print('All modules and packages successfully loaded.')\n",
    "\n",
    "# Bring in the CSV as a data frame\n",
    "# NOTE: these documents have a lot of analysis added to them, between the original download from the\n",
    "# CSO website and the working copy. Things added are \n",
    "obs_tp_2018 = pd.read_csv('csvs/2018_obs_TP_working.csv', encoding = \"ISO-8859-1\")\n",
    "obs_tp_2017 = pd.read_csv('csvs/2017_obs_TP_working.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "########################################################\n",
    "######### USER INPUT REQUIRED  ######################\n",
    "########################################################\n",
    "\n",
    "# Choose the water year for the analysis\n",
    "df = obs_tp_2017\n",
    "\n",
    "# Change date here to match the year above\n",
    "y2 = 2017\n",
    "y1 = y2 - 1\n",
    "\n",
    "########################################################\n",
    "############    END USER INPUT      ######################\n",
    "########################################################\n",
    "\n",
    "# Subset by seasons/time, elevation, etc.\n",
    "# These subsets are specific to the domain and types of \n",
    "# CSO observations that were submitted. \n",
    "early = df[df.Y == y1]\n",
    "late = df[df.Y == y2]\n",
    "dec = df[df.M == 12]\n",
    "jan = df[df.M == 1]\n",
    "feb = df[df.M == 2]\n",
    "mar = df[df.M == 3]\n",
    "apr = df[df.M == 4]\n",
    "may = df[df.M == 5]\n",
    "jun = df[df.M == 6]\n",
    "winter = df[df.M.gt(0) & df.M.lt(5)]\n",
    "spring = df[df.M.gt(3) & df.M.lt(6)]\n",
    "peak = late[(late.M.gt(3) & late.D.gt(14) | late.M.gt(4))]\n",
    "\n",
    "# Subset by elevation bands\n",
    "high_elev = df[df.elevation >= 1000]\n",
    "low_elev = df[df.elevation <= 1000]\n",
    "elev_band_0250 = df[df.elevation <= 250]\n",
    "elev_band_0500 = df[df.elevation.lt(500) & df.elevation.gt(251)]\n",
    "elev_band_0750 = df[df.elevation.lt(750) & df.elevation.gt(501)]\n",
    "elev_band_1000 = df[df.elevation.lt(1000) & df.elevation.gt(751)]\n",
    "elev_band_1250 = df[df.elevation.lt(1250) & df.elevation.gt(1001)]\n",
    "elev_band_gt1250 = df[df.elevation.gt(1250)]\n",
    "\n",
    "spring_lt1000 = df[df.M.gt(3) & df.M.lt(6) & df.elevation.lt(1000)]\n",
    "spring_gt1000 = df[df.M.gt(3) & df.M.lt(6) & df.elevation.gt(1000)]\n",
    "\n",
    "# Sort for types of ordering, ascending by elevation and date\n",
    "asc_elev = df.sort_values(by=['elevation'])\n",
    "asc_date = df.sort_values(by=['Y','M','D'])\n",
    "\n",
    "# Print some details about the data set in all of the categories\n",
    "print((len(high_elev)),'=high')\n",
    "print((len(low_elev)),'=low')\n",
    "print((len(elev_band_0250)), '=band_0250')\n",
    "print((len(elev_band_0500)), '=band_0500')\n",
    "print((len(elev_band_0750)), '=band_0750')\n",
    "print((len(elev_band_1000)), '=band_1000')\n",
    "print((len(elev_band_1250)), '=band_1250')\n",
    "print((len(elev_band_gt1250)), '=band_gt1250')\n",
    "print((len(early)),'=early')\n",
    "print((len(late)),'=late')\n",
    "print((len(winter)),'=winter')\n",
    "print((len(spring)),'=spring')\n",
    "print((len(peak)),'=peak')\n",
    "print((len(spring_lt1000)),'=sp1000')\n",
    "print((len(spring_gt1000)),'=spgt1000')\n",
    "print((len(apr)),'=apr')\n",
    "print((len(may)),'=may')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files successfully created\n"
     ]
    }
   ],
   "source": [
    "# This is the loop that randomly samples the previously subsetted dataset, and chooses\n",
    "# a smaller subset of those data to create a csv for the assimilation input.\n",
    "# OUTPUT: a separate .dat file for each of the new, randomly selected swe values/locations within the subsets.\n",
    "# These files are in the exact format required for SnowAssim inputs.\n",
    "\n",
    "########################################################\n",
    "######### USER INPUT REQUIRED  ######################\n",
    "########################################################\n",
    "# Num_runs is the number of times the random sampling of the subsetted data is repeated.\n",
    "num_runs = 10\n",
    "\n",
    "# Num_obs is the number of CSO obs to choose within the subset\n",
    "num_obs = 16\n",
    "\n",
    "# This is the subset (defined in the cell above) we that we want to randomly sample.\n",
    "subset_df = peak\n",
    "\n",
    "########################################################\n",
    "############    END USER INPUT      ######################\n",
    "########################################################\n",
    "\n",
    "# This creates a list from the index of the subset. \n",
    "sublist = subset_df.index.tolist()\n",
    "\n",
    "# Uncomment to learn some things about the sublist\n",
    "# print(sublist)\n",
    "# first = sublist[0]\n",
    "# print(first, 'subset first index')\n",
    "# num = len(sublist)-1\n",
    "# print(num, 'length of sublist')\n",
    "# last = sublist[num]\n",
    "# print(last,'subset last index')\n",
    "\n",
    "# This takes the num and increments from 1 to num\n",
    "for i in range(1,num_runs+1):\n",
    "    # Chooses a random set of row id numbers from the index of the dataframe (subset_df)\n",
    "    rand_rows = random.sample(sublist, num_obs)\n",
    "    # Create a new subsetted dataframe with only the new randomly chosen rows\n",
    "    new_df = subset_df.loc[rand_rows].sort_values(by=['Y','M','D']).round({'albersX': 3,'albersY': 3,'SWE_hill':3,'SWE_sturm':3})\n",
    "    # Group the data using the date_agg column \n",
    "    dates = new_df.groupby(by='date_agg2')\n",
    "    # Count the number of different dates from the random set\n",
    "    num_dates = new_df.groupby(by='date_agg2').size()\n",
    "    \n",
    "    # Open a file and print the data to the file\n",
    "    #OUTPUT of for loop = a set of input files for SnowAssim, with randomly sampled rows from the subset df\n",
    "    # SWE values using Hill et al. 2019\n",
    "    with open('subset'+str(num_obs)+'_hill_'+str(i)+'.dat','w') as file1:\n",
    "        file1.write(str(len(num_dates.index))+'\\n')\n",
    "        for j in range(0,len(num_dates)):\n",
    "            temp = new_df[new_df.date_agg2==num_dates.index[j]]\n",
    "            # Access the SWE values estimated by the Hill method.\n",
    "            condense_hill = temp[['albersX','albersY','SWE_hill']].sort_index()\n",
    "            file1.write(str(num_dates.index[j])+'\\n')\n",
    "            file1.write(str(num_dates[j])+'\\n')\n",
    "            file1.write(condense_hill.to_string(header=False)+'\\n') \n",
    "    file1.close()\n",
    "    \n",
    "    # Open a file and print the data to the file\n",
    "    #OUTPUT of for loop = a set of input files for SnowAssim, with randomly sampled rows from the subset df\n",
    "    # SWE values using Sturm et al. 2010\n",
    "    with open('subset'+str(num_obs)+'_sturm_'+str(i)+'.dat','w') as f:\n",
    "        line0 =str(len(num_dates.index))+'\\n'\n",
    "        f.write(line0)\n",
    "        for k in range(0,len(num_dates)):\n",
    "            line1 = str(num_dates.index[k])+'\\n'\n",
    "            line2 = str(num_dates[k])+'\\n'\n",
    "            temp = new_df[new_df.date_agg2==num_dates.index[k]]\n",
    "            # Access the SWE values estimated by the Sturm method.\n",
    "            condense_sturm = temp[['albersX','albersY','SWE_sturm']].sort_index()\n",
    "            f.write(line1)\n",
    "            f.write(line2)\n",
    "            f.write(condense_sturm.to_string(header=False)+'\\n') \n",
    "    f.close()\n",
    "    \n",
    "print('Files successfully created')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
